[{"title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective", "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within\ntheir parameters through pre-training on extensive corpora. While prior\nresearch has delved into operations on these parameters to manipulate the\nunderlying implicit knowledge (encompassing detection, editing, and merging),\nthere remains an ambiguous understanding regarding their transferability across\nmodels with varying scales. In this paper, we seek to empirically investigate\nknowledge transfer from larger to smaller models through a parametric\nperspective. To achieve this, we employ sensitivity-based techniques to extract\nand align knowledge-specific parameters between different LLMs. Moreover, the\nLoRA module is used as the intermediary mechanism for injecting the extracted\nknowledge into smaller models. Evaluations across four benchmarks validate the\nefficacy of our proposed method. Our findings highlight the critical factors\ncontributing to the process of parametric knowledge transfer, underscoring the\ntransferability of model parameters across LLMs of different scales. We release\ncode and data at \\url{https://github.com/maszhongming/ParaKnowTransfer}.", "authors": ["Ming Zhong", "Chenxin An", "Weizhu Chen", "Jiawei Han", "Pengcheng He"], "url": "http://arxiv.org/abs/2310.11451v1"}, {"title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models", "abstract": "The vision and language generative models have been overgrown in recent\nyears. For video generation, various open-sourced models and public-available\nservices are released for generating high-visual quality videos. However, these\nmethods often use a few academic metrics, for example, FVD or IS, to evaluate\nthe performance. We argue that it is hard to judge the large conditional\ngenerative models from the simple metrics since these models are often trained\non very large datasets with multi-aspect abilities. Thus, we propose a new\nframework and pipeline to exhaustively evaluate the performance of the\ngenerated videos. To achieve this, we first conduct a new prompt list for\ntext-to-video generation by analyzing the real-world prompt list with the help\nof the large language model. Then, we evaluate the state-of-the-art video\ngenerative models on our carefully designed benchmarks, in terms of visual\nqualities, content qualities, motion qualities, and text-caption alignment with\naround 18 objective metrics. To obtain the final leaderboard of the models, we\nalso fit a series of coefficients to align the objective metrics to the users'\nopinions. Based on the proposed opinion alignment method, our final score shows\na higher correlation than simply averaging the metrics, showing the\neffectiveness of the proposed evaluation method.", "authors": ["Yaofang Liu", "Xiaodong Cun", "Xuebo Liu", "Xintao Wang", "Yong Zhang", "Haoxin Chen", "Yang Liu", "Tieyong Zeng", "Raymond Chan", "Ying Shan"], "url": "http://arxiv.org/abs/2310.11440v1"}, {"title": "Identifying Interpretable Visual Features in Artificial and Biological Neural Systems", "abstract": "Single neurons in neural networks are often ``interpretable'' in that they\nrepresent individual, intuitively meaningful features. However, many neurons\nexhibit $\\textit{mixed selectivity}$, i.e., they represent multiple unrelated\nfeatures. A recent hypothesis proposes that features in deep networks may be\nrepresented in $\\textit{superposition}$, i.e., on non-orthogonal axes by\nmultiple neurons, since the number of possible interpretable features in\nnatural data is generally larger than the number of neurons in a given network.\nAccordingly, we should be able to find meaningful directions in activation\nspace that are not aligned with individual neurons. Here, we propose (1) an\nautomated method for quantifying visual interpretability that is validated\nagainst a large database of human psychophysics judgments of neuron\ninterpretability, and (2) an approach for finding meaningful directions in\nnetwork activation space. We leverage these methods to discover directions in\nconvolutional neural networks that are more intuitively meaningful than\nindividual neurons, as we confirm and investigate in a series of analyses.\nMoreover, we apply the same method to two recent datasets of visual neural\nresponses in the brain and find that our conclusions largely transfer to real\nneural data, suggesting that superposition might be deployed by the brain. This\nalso provides a link with disentanglement and raises fundamental questions\nabout robust, efficient and factorized representations in both artificial and\nbiological neural systems.", "authors": ["David Klindt", "Sophia Sanborn", "Francisco Acosta", "Fr\u00e9d\u00e9ric Poitevin", "Nina Miolane"], "url": "http://arxiv.org/abs/2310.11431v1"}]